# cloudnative-pg

## Postgres Clusters

**Main components for applications:**

### For production (choose workflow):

**Option A: New database (no existing backup)**
1. Start with: `../../../components/cnpg/initdb` - Creates empty database
2. After first successful backup: Switch to `../../../components/cnpg/restore` ‚úÖ **Keep permanently**

**Option B: Restore existing database**
1. Use: `../../../components/cnpg/restore` ‚úÖ **Keep permanently**

### Why use `restore` permanently?

‚úÖ **Disaster recovery:** If cluster is deleted/corrupted, it will automatically restore from latest backup
‚úÖ **Infrastructure as code:** Cluster recreate = automatic restore, no manual intervention
‚úÖ **Production safety:** Always recovers to known good state

**Component structure (internal):**
```
base ‚Üí initdb (one-time: create empty DB)
     ‚Üí restore (permanent: restore from backup)
```

- `base` - Core cluster + backup configuration (Cluster, ExternalSecret, ObjectStore, ScheduledBackup, WAL archiving via plugins)
- `initdb` - Adds bootstrap.initdb patch ‚ö†Ô∏è **Temporary - switch to restore after first backup**
- `restore` - Adds bootstrap.recovery patch + externalClusters ‚úÖ **Permanent for production**

## Prerequisites

**Bitwarden secret:** `cloudnative-pg` with fields:
- `POSTGRES_SUPER_USER` - PostgreSQL superuser username (default: `postgres`)
- `POSTGRES_SUPER_PASS` - PostgreSQL superuser password (strong random password)
- `AWS_ACCESS_KEY_ID` - S3/MinIO access key for backups
- `AWS_SECRET_ACCESS_KEY` - S3/MinIO secret key for backups

## How It Works

**Application credentials (auto-generated by CNPG):**
- Username: `${APP}` (e.g., `coder`, `grafana`, `authentik`)
- Password: **Automatically generated** by CNPG during bootstrap
- Database: `${APP}` (can override with `CNPG_DATABASE` variable)

**CNPG automatically creates:**
1. Application user with secure random password
2. Application database owned by that user
3. Kubernetes secret `postgres-${APP}-app` containing credentials

**No init container needed!** The `initdb` bootstrap handles everything.

**Application connection (easiest way - use pre-built URI):**
```yaml
env:
  - name: DATABASE_URL
    valueFrom:
      secretKeyRef:
        name: postgres-${APP}-app
        key: uri  # Contains full connection string
```

**Alternative (if you need individual fields):**
```yaml
env:
  - name: DB_HOST
    valueFrom:
      secretKeyRef:
        name: postgres-${APP}-app
        key: host
  - name: DB_PASSWORD
    valueFrom:
      secretKeyRef:
        name: postgres-${APP}-app
        key: password
  # Also available: username, dbname, port, pgpass, jdbc-uri
```

### Variables to set:

```yaml
  dependsOn:
    - name: cloudnative-pg
      namespace: database
    - name: plugin-barman-cloud
      namespace: database
  postBuild:
    substitute:
      APP: *app # required
      CNPG_REPLICAS: '3' # default: 3 for proper HA quorum (same as CPGO)
      CNPG_IMAGE: ghcr.io/cloudnative-pg/postgresql:17.6-standard-trixie@sha256:e185037ad4c926fece1d3cfd1ec99680a862d7d02b160354e263a04a2d46b5f5 # default, can override
      CNPG_SIZE: 2Gi # default for small DBs (<500MB), use 5Gi for large (>2GB)
      CNPG_STORAGECLASS: local-hostpath # default
      CNPG_REQUESTS_CPU: 100m # default for small DBs, use 500m for large
      CNPG_REQUESTS_MEMORY: 256Mi # default, scales with CNPG_LIMITS_MEMORY
      CNPG_LIMITS_MEMORY: 512Mi # default for small DBs, use 1-2Gi for larger
      CNPG_MAX_CONNECTIONS: '100' # default (reduced from 600 - overkill for most apps)
      CNPG_SHARED_BUFFERS: 128MB # default (~25% of memory), use 256-512MB for larger
      CNPG_DISABLED_SERVICES: "['ro', 'r']" # default: disable read-only services

      # Optional PostgreSQL tuning (all have sensible defaults)
      CNPG_WORK_MEM: 4MB # memory for sort operations
      CNPG_MAINTENANCE_WORK_MEM: 64MB # memory for maintenance (VACUUM, CREATE INDEX)
      CNPG_EFFECTIVE_CACHE_SIZE: 512MB # expected OS cache size (~75% of memory)
      CNPG_RANDOM_PAGE_COST: '1.1' # SSD optimized (default 4.0 for HDD)
      CNPG_WAL_BUFFERS: 16MB # WAL buffer size

      # Synchronous replication - DEFAULT RECOMMENDED for all HA clusters (3 replicas)
      # Provides near-zero data loss with minimal performance impact
      CNPG_SYNC_METHOD: any # 'any' = quorum (default), 'first' = priority, '' = async only
      CNPG_SYNC_NUMBER: '1' # minimum standby replicas for sync (1 = wait for 1 standby)
      CNPG_SYNC_DURABILITY: preferred # 'preferred' = self-healing (recommended), 'required' = strict RPO=0
```

**Deployment profiles:**

**Profile 1: HA with sync replication (RECOMMENDED for production):**
```yaml
CNPG_REPLICAS: '3'                    # 3 nodes for quorum
CNPG_SYNC_METHOD: any                 # enable sync replication
CNPG_SYNC_NUMBER: '1'                 # wait for 1 standby
CNPG_SYNC_DURABILITY: preferred       # self-healing mode
```
‚úÖ Near-zero data loss, automatic failover, self-healing
üéØ Use for: authentik, grafana, *arr apps, any production data

**Profile 2: HA async only (legacy/testing):**
```yaml
CNPG_REPLICAS: '3'                    # 3 nodes for HA
CNPG_SYNC_METHOD: ''                  # disable sync (async only)
```
‚ö†Ô∏è Potential few seconds data loss on failover
üéØ Use for: temporary/test clusters, truly idle DBs

**Profile 3: Single-node (dev/testing only):**
```yaml
CNPG_REPLICAS: '1'                    # single node
CNPG_SYNC_METHOD: ''                  # must be empty (no standbys)
```
‚ùå No HA, no replication, single point of failure
üéØ Use for: development, testing, non-critical temporary data

**Resource sizing guide:**

Default: **Burstable QoS** - low requests, can burst when needed (ideal for idle databases!)
- Requests: CPU 25m, Memory 256Mi (minimal reservation)
- Limits: Memory 512Mi (can burst 2x), CPU unlimited (can use more when available)

**Custom sizing (override with variables):**
- **Guaranteed QoS** (for critical workloads): Set `CNPG_CPU` + `CNPG_MEMORY` (both requests=limits)
- **Small DBs**: `CNPG_REQUESTS_CPU: 50m, CNPG_LIMITS_MEMORY: 512Mi` (default)
- **Medium DBs**: `CNPG_REQUESTS_CPU: 100m, CNPG_REQUESTS_MEMORY: 512Mi, CNPG_LIMITS_MEMORY: 1Gi`
- **Large DBs**: `CNPG_REQUESTS_CPU: 200m, CNPG_REQUESTS_MEMORY: 1Gi, CNPG_LIMITS_MEMORY: 2Gi`

üí° **Why Burstable?** Idle DBs mostly sit at low usage but need bursts for VACUUM, backups, indexing.

**High Availability & Failover:**
- ‚úÖ Automatic failover built-in - operator promotes standby with lowest lag
- ‚úÖ Applications use `-rw` service - automatically updated to new primary
- ‚úÖ No PgBouncer pooler needed for idle databases with few connections
- ‚úÖ **Synchronous replication recommended by default** - minimal overhead, near-zero data loss
- üìö [Failure modes docs](https://cloudnative-pg.io/documentation/current/failure_modes/)

**Why sync replication for ALL production apps?**
- üí∞ **Free protection**: You already have 3 replicas - use them!
- ‚ö° **Low overhead**: Write latency increase ~1-2ms (waiting for 1 standby ACK)
- üõ°Ô∏è **Data safety**: Near RPO=0 - no data loss on failover
- üîÑ **Self-healing**: `preferred` mode = cluster stays writable even if standbys down
- üéØ **Worth it**: For arr apps (library metadata), authentik (sessions), grafana (dashboards)

**Fine-tuning options:**
- **`dataDurability: preferred`** (default, recommended): Degrades to async if standbys unavailable ‚Üí cluster stays writable
- **`dataDurability: required`**: Strict RPO=0, but writes STOP if standbys down ‚Üí only for critical financial/compliance data
- **`CNPG_SYNC_METHOD: ''`** (empty): Disable sync completely ‚Üí pure async replication (for truly idle test DBs)

**See also:** `/docs/cnpg-resource-configs.yaml` for per-cluster recommendations based on actual usage analysis.
```

---

## Usage Examples

### Workflow 1: New database from scratch

```yaml
# kubernetes/apps/<namespace>/<app>/ks.yaml
# Step 1: Initial deployment with initdb
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: miniflux
spec:
  components:
    - ../../../components/cnpg/initdb  # ‚ö†Ô∏è Temporary - creates empty DB
  dependsOn:
    - name: cloudnative-pg
      namespace: database
    - name: plugin-barman-cloud
      namespace: database
  postBuild:
    substitute:
      APP: miniflux
      CNPG_SIZE: 2Gi

# Step 2: After ~24h (first backup completed), switch to restore
spec:
  components:
    - ../../../components/cnpg/restore  # ‚úÖ Keep this permanently!
```

### Workflow 2: Restore existing database

```yaml
# kubernetes/apps/<namespace>/<app>/ks.yaml
# Use restore from the start if backup already exists
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: authentik
spec:
  components:
    - ../../../components/cnpg/restore  # ‚úÖ Keep permanently
  postBuild:
    substitute:
      APP: authentik
      CNPG_SIZE: 5Gi
```

**This is the permanent configuration for production!**

```yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: miniflux-db
spec:
  path: ./kubernetes/apps/self-hosted/miniflux
  dependsOn:
    - name: cloudnative-pg
  postBuild:
    substitute:
      APP: miniflux
      CNPG_SIZE: 2Gi
  components:
    - ../../../components/cnpg/backup
```

### Large database with custom tuning

```yaml
# kubernetes/apps/security/authentik/ks.yaml
components:
  - ../../../components/cnpg/initdb
postBuild:
  substitute:
    APP: authentik
    CNPG_REPLICAS: '3'
    CNPG_SIZE: 5Gi
    CNPG_REQUESTS_CPU: 500m
    CNPG_LIMITS_MEMORY: 2Gi
    CNPG_MAX_CONNECTIONS: '300'
    CNPG_SHARED_BUFFERS: 512MB
    CNPG_EFFECTIVE_CACHE_SIZE: 1536MB
    CNPG_WORK_MEM: 16MB
```

---

## Component Details

### `restore` ‚úÖ **Main component for production**

**Use for:** All production databases with backups

**What it includes:**
- `base` component (cluster + external secret + S3 backup config)
- `restore` patch (adds `spec.bootstrap.recovery` + `externalClusters`)

**Behavior:**
- **First deploy:** Restores from latest S3 backup
- **Normal operation:** Cluster runs normally, backups continue
- **After cluster deletion:** Recreate automatically restores from latest backup

**Keep permanently!** This provides disaster recovery protection.

---

### `initdb` ‚ö†Ô∏è **Temporary component for new databases**

**Use for:** Brand new databases (no existing backup)

**What it includes:**
- `base` component (cluster + external secret + S3 backup config)
- `initdb` patch (adds `spec.bootstrap.initdb`)

**Generated resources:**
- Cluster: `postgres-${APP}`
- Secret: `postgres-${APP}-app` (auto-generated credentials)
- Empty database owned by app user

**‚ö†Ô∏è Switch to `restore` after first successful backup!** (typically after 24h)

Why switch? If you ever need to recreate the cluster, you want it to restore from backup, not start empty.

---

## FAQ

**Q: Should I keep `restore` or `initdb` permanently?**
A: **Always use `restore` for production!** Use `initdb` only temporarily for brand new databases, then switch to `restore` after first backup.

**Q: What if I keep `initdb` permanently?**
A: Bad idea! If your cluster gets deleted, it will recreate with an EMPTY database instead of restoring from backup. You'll lose all data.

**Q: Does `restore` component restore on every Flux reconcile?**
A: No! `bootstrap.recovery` only runs when the cluster is first created. After that, it's ignored and cluster operates normally.

**Q: What if backup doesn't exist yet when using `restore`?**
A: The cluster will fail to start. That's why you use `initdb` first for new databases, wait for first backup, then switch to `restore`.

**Q: Can I change from `initdb` to `restore` without downtime?**
A: Yes! After the cluster is running and has backups, switching components is just a manifest change. The running cluster is unaffected - it only matters when cluster is recreated.

**Q: How do I know when to switch from `initdb` to `restore`?**
A: After 24 hours (one scheduled backup completed). Check: `kubectl get backup -n <namespace>` to see if backup exists.
  - ../../../components/cnpg/backup
```

### Restore from backup

This is covered in Workflow 2 above - just use `restore` component from the start and keep it permanently.

**What happens:**
- First deploy: Restores from latest backup in S3
- Subsequent deploys: Cluster already exists, no restore happens
- Cluster deleted & redeployed: Automatically restores from latest backup again

**This is the desired behavior for production!**

---

## HealthChecks

```yaml
  healthChecks:
    - apiVersion: &postgresVersion postgresql.cnpg.io/v1
      kind: &postgresKind Cluster
      name: postgres-APPNAME
      namespace: *namespace
  healthCheckExprs:
    - apiVersion: *postgresVersion
      kind: *postgresKind
      failed: status.conditions.filter(e, e.type == 'Ready').all(e, e.status == 'False')
      current: status.conditions.filter(e, e.type == 'Ready').all(e, e.status == 'True')
```
