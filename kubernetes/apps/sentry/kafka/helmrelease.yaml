---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kafka
spec:
  interval: 1h
  chartRef:
    kind: OCIRepository
    name: app-template
  values:
    controllers:
      kafka:
        annotations:
          reloader.stakater.com/auto: "true"
        initContainers:
          init-lost-found:
            image:
              repository: busybox
              tag: latest
            command:
              - sh
              - -c
              - rm -rf /var/lib/kafka/data/lost+found
            securityContext:
              runAsUser: 0
        containers:
          app:
            image:
              repository: confluentinc/cp-kafka
              tag: 7.9.5@sha256:c4c6b755551da17fff056b9c8b39700f99020083bd2d69a171ece4784f33e640
            env:
              # KRaft mode (no Zookeeper)
              KAFKA_PROCESS_ROLES: "broker,controller"
              KAFKA_NODE_ID: "1001"
              CLUSTER_ID: "MkU3OEVBNTcwNTJENDM2Qk"
              KAFKA_CONTROLLER_QUORUM_VOTERS: "1001@localhost:29093"
              KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
              # Listeners
              KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:29093"
              KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka.sentry.svc.cluster.local:9092"
              KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT"
              KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
              # Topic settings (single node)
              KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
              KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS: "1"
              KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
              KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"
              KAFKA_DEFAULT_REPLICATION_FACTOR: "1"
              KAFKA_MIN_INSYNC_REPLICAS: "1"
              # Auto-create topics (Sentry creates many topics)
              KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
              # Performance
              KAFKA_LOG_RETENTION_HOURS: "72"
              KAFKA_COMPRESSION_TYPE: "lz4"
              # Message size (50MB for large replay recordings)
              KAFKA_MESSAGE_MAX_BYTES: "50000000"
              KAFKA_MAX_REQUEST_SIZE: "50000000"
              KAFKA_REPLICA_FETCH_MAX_BYTES: "50000000"
              # Disable telemetry
              CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
              # Logging
              KAFKA_LOG4J_LOGGERS: "kafka.cluster=WARN,kafka.controller=WARN,kafka.coordinator=WARN,kafka.log=WARN,kafka.server=WARN,state.change.logger=WARN"
              KAFKA_LOG4J_ROOT_LOGLEVEL: "WARN"
              KAFKA_TOOLS_LOG4J_LOGLEVEL: "WARN"
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  exec:
                    command:
                      - sh
                      - -c
                      - nc -z localhost 9092
                  initialDelaySeconds: 30
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 6
              readiness:
                enabled: true
                custom: true
                spec:
                  exec:
                    command:
                      - sh
                      - -c
                      - nc -z localhost 9092
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 3
              startup:
                enabled: true
                custom: true
                spec:
                  exec:
                    command:
                      - sh
                      - -c
                      - nc -z localhost 9092
                  initialDelaySeconds: 10
                  periodSeconds: 5
                  timeoutSeconds: 5
                  failureThreshold: 30
            resources:
              requests:
                cpu: 200m
                memory: 1Gi
              limits:
                memory: 4Gi
      # One-time job to create all Sentry topics with proper configs
      topics-init:
        type: job
        annotations:
          helm.sh/hook: post-install,post-upgrade
          helm.sh/hook-weight: "1"
          helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
        containers:
          init:
            image:
              repository: confluentinc/cp-kafka
              tag: 7.9.5@sha256:c4c6b755551da17fff056b9c8b39700f99020083bd2d69a171ece4784f33e640
            command:
              - /bin/bash
              - -c
              - |
                set -e
                BOOTSTRAP="kafka.sentry.svc.cluster.local:9092"

                # Wait for Kafka to be ready
                echo "Waiting for Kafka to be ready..."
                until kafka-topics --bootstrap-server $$BOOTSTRAP --list >/dev/null 2>&1; do
                  echo "Kafka not ready, waiting..."
                  sleep 5
                done
                echo "Kafka is ready!"

                create_topic() {
                  local name=$$1
                  shift
                  local config_args=()
                  while [[ $$# -gt 0 ]]; do
                    config_args+=("--config" "$$1")
                    shift
                  done

                  if kafka-topics --bootstrap-server $$BOOTSTRAP --describe --topic "$$name" >/dev/null 2>&1; then
                    echo "Topic $$name already exists, skipping"
                  else
                    echo "Creating topic: $$name"
                    kafka-topics --bootstrap-server $$BOOTSTRAP --create --topic "$$name" \
                      --partitions 1 --replication-factor 1 "$${config_args[@]}" 2>/dev/null || true
                  fi
                }

                # Core events topics
                create_topic "events" "message.timestamp.type=LogAppendTime"
                create_topic "event-replacements"
                create_topic "snuba-commit-log" "cleanup.policy=compact,delete" "min.compaction.lag.ms=3600000"
                create_topic "cdc"
                create_topic "transactions" "message.timestamp.type=LogAppendTime"
                create_topic "snuba-transactions-commit-log" "cleanup.policy=compact,delete" "min.compaction.lag.ms=3600000"

                # Metrics topics
                create_topic "snuba-metrics" "message.timestamp.type=LogAppendTime"
                create_topic "snuba-metrics-commit-log" "cleanup.policy=compact,delete" "min.compaction.lag.ms=3600000"
                create_topic "snuba-generic-metrics" "message.timestamp.type=LogAppendTime"
                create_topic "snuba-generic-metrics-sets-commit-log" "cleanup.policy=compact,delete" "min.compaction.lag.ms=3600000"
                create_topic "snuba-generic-metrics-distributions-commit-log" "cleanup.policy=compact,delete" "min.compaction.lag.ms=3600000"
                create_topic "snuba-generic-metrics-counters-commit-log" "cleanup.policy=compact,delete" "min.compaction.lag.ms=3600000"
                create_topic "snuba-generic-metrics-gauges-commit-log" "cleanup.policy=compact,delete" "min.compaction.lag.ms=3600000"

                # Generic events
                create_topic "generic-events" "message.timestamp.type=LogAppendTime"
                create_topic "snuba-generic-events-commit-log" "cleanup.policy=compact,delete" "min.compaction.lag.ms=3600000"
                create_topic "group-attributes" "message.timestamp.type=LogAppendTime"

                # Outcomes
                create_topic "outcomes"
                create_topic "outcomes-dlq"
                create_topic "outcomes-billing"
                create_topic "outcomes-billing-dlq"

                # Ingest topics
                create_topic "ingest-events"
                create_topic "ingest-events-dlq"
                create_topic "ingest-spans"
                create_topic "ingest-sessions"
                create_topic "ingest-attachments"
                create_topic "ingest-attachments-dlq"
                create_topic "ingest-transactions"
                create_topic "ingest-transactions-dlq"
                create_topic "ingest-transactions-backlog"
                create_topic "ingest-replay-events" "message.timestamp.type=LogAppendTime" "max.message.bytes=15000000"
                create_topic "ingest-replay-recordings"
                create_topic "ingest-metrics"
                create_topic "ingest-metrics-dlq"
                create_topic "ingest-performance-metrics"
                create_topic "ingest-feedback-events"
                create_topic "ingest-feedback-events-dlq"
                create_topic "ingest-monitors"
                create_topic "ingest-occurrences"

                # Subscriptions
                create_topic "scheduled-subscriptions-events"
                create_topic "scheduled-subscriptions-transactions"
                create_topic "scheduled-subscriptions-metrics"
                create_topic "scheduled-subscriptions-generic-metrics-sets"
                create_topic "scheduled-subscriptions-generic-metrics-distributions"
                create_topic "scheduled-subscriptions-generic-metrics-counters"
                create_topic "scheduled-subscriptions-generic-metrics-gauges"
                create_topic "scheduled-subscriptions-eap-spans"
                create_topic "events-subscription-results"
                create_topic "transactions-subscription-results"
                create_topic "metrics-subscription-results"
                create_topic "generic-metrics-subscription-results"
                create_topic "eap-spans-subscription-results"
                create_topic "subscription-results-eap-items"

                # Snuba misc
                create_topic "snuba-queries" "message.timestamp.type=LogAppendTime"
                create_topic "snuba-spans"
                create_topic "snuba-eap-spans-commit-log"
                create_topic "snuba-items-commit-log"
                create_topic "snuba-eap-mutations"
                create_topic "snuba-lw-deletions-generic-events"
                create_topic "snuba-profile-chunks"
                create_topic "snuba-uptime-results"
                create_topic "snuba-ourlogs"
                create_topic "snuba-items"

                # Dead letter queues
                create_topic "snuba-dead-letter-metrics"
                create_topic "snuba-dead-letter-generic-metrics"
                create_topic "snuba-dead-letter-replays"
                create_topic "snuba-dead-letter-generic-events"
                create_topic "snuba-dead-letter-querylog"
                create_topic "snuba-dead-letter-group-attributes"

                # Profiles
                create_topic "processed-profiles" "message.timestamp.type=LogAppendTime"
                create_topic "profiles-call-tree"
                create_topic "profiles"

                # Monitors
                create_topic "monitors-clock-tasks"
                create_topic "monitors-clock-tick"
                create_topic "monitors-incident-occurrences"

                # Uptime
                create_topic "uptime-configs"
                create_topic "uptime-results"

                # Buffered segments
                create_topic "buffered-segments"
                create_topic "buffered-segments-dlq"

                # Shared resources
                create_topic "shared-resources-usage"

                # Task worker topics
                create_topic "task-worker"
                create_topic "taskworker"
                create_topic "taskworker-dlq"
                create_topic "taskworker-billing"
                create_topic "taskworker-billing-dlq"
                create_topic "taskworker-control"
                create_topic "taskworker-control-dlq"
                create_topic "taskworker-ingest"
                create_topic "taskworker-ingest-dlq"
                create_topic "taskworker-ingest-errors"
                create_topic "taskworker-ingest-errors-dlq"
                create_topic "taskworker-ingest-transactions"
                create_topic "taskworker-ingest-transactions-dlq"
                create_topic "taskworker-internal"
                create_topic "taskworker-internal-dlq"
                create_topic "taskworker-limited"
                create_topic "taskworker-limited-dlq"
                create_topic "taskworker-long"
                create_topic "taskworker-long-dlq"
                create_topic "taskworker-products"
                create_topic "taskworker-products-dlq"
                create_topic "taskworker-sentryapp"
                create_topic "taskworker-sentryapp-dlq"
                create_topic "taskworker-symbolication"
                create_topic "taskworker-symbolication-dlq"
                create_topic "taskworker-usage"
                create_topic "taskworker-usage-dlq"

                echo "All topics created successfully!"
                echo "Total topics: $$(kafka-topics --bootstrap-server $$BOOTSTRAP --list | wc -l)"
            resources:
              requests:
                cpu: 50m
                memory: 256Mi
              limits:
                memory: 512Mi
    defaultPodOptions:
      priorityClassName: cluster-low
      securityContext:
        runAsNonRoot: false
        runAsUser: 1000
        runAsGroup: 100
        fsGroup: 100
        fsGroupChangePolicy: OnRootMismatch
      # Only on workers, prefer different node than ClickHouse
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/control-plane
                    operator: DoesNotExist
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: clickhouse
                topologyKey: kubernetes.io/hostname
    service:
      app:
        controller: kafka
        ports:
          kafka:
            port: 9092
    persistence:
      data:
        existingClaim: kafka
        advancedMounts:
          kafka:
            app:
              - path: /var/lib/kafka/data
      log:
        type: emptyDir
        advancedMounts:
          kafka:
            app:
              - path: /var/lib/kafka/log
      secrets:
        type: emptyDir
        advancedMounts:
          kafka:
            app:
              - path: /etc/kafka/secrets
