---
# yaml-language-server: $schema=https://schemas.vzkn.eu/postgresql.cnpg.io/cluster_v1.json
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: &name postgres-${APP}
spec:
  # INSTANCES configuration:
  # - Multi-node cluster: 3+ (HA with zero-downtime updates)
  # - Single-node cluster: 2 recommended (zero-downtime updates, read scaling)
  # - Single-node minimal: 1 (expect 5-15s downtime on updates)
  instances: ${CNPG_REPLICAS:=3}
  imageName: ${CNPG_IMAGE:=ghcr.io/cloudnative-pg/postgresql}:${CNPG_VERSION:=18.1-system-trixie}
  # primaryUpdateMethod:
  # - switchover: Zero-downtime (requires instances >= 2)
  # - restart: Fast but 5-15s downtime (works with instances = 1)
  primaryUpdateMethod: switchover
  primaryUpdateStrategy: unsupervised
  plugins:
    - name: barman-cloud.cloudnative-pg.io
      isWALArchiver: true
      parameters:
        barmanObjectName: *name
        serverName: *name
  managed:
    services:
      disabledDefaultServices: ${CNPG_DISABLED_SERVICES:=['ro', 'r']}
  storage:
    size: ${CNPG_SIZE:=2Gi}
    storageClass: ${CNPG_STORAGECLASS:=ceph-block-single}
  superuserSecret:
    name: ${APP}-cnpg-secret
  enableSuperuserAccess: true
  # Bootstrap method must be provided by a component (initdb or restore)
  # AFFINITY configuration - controls pod placement across nodes
  # For SINGLE-NODE clusters with 2+ instances:
  #   Option 1: Remove entire 'affinity:' section (simplest)
  #   Option 2: Change podAntiAffinityType to 'preferred' (allows same-node placement)
  #   Option 3: Use CNPG_ANTI_AFFINITY variable in ks.yaml: CNPG_ANTI_AFFINITY: preferred
  affinity:
    enablePodAntiAffinity: true
    topologyKey: kubernetes.io/hostname
    # required: Hard constraint - instances MUST be on different nodes (blocks single-node with 2+ instances)
    # preferred: Soft constraint - prefers different nodes but allows same-node if needed
    podAntiAffinityType: required
    nodeSelector:
      kubernetes.io/arch: amd64
      # Schedule on workers
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #       - matchExpressions:
      #           - key: node-role.kubernetes.io/control-plane
      #             operator: DoesNotExist
  postgresql:
    # SYNCHRONOUS REPLICATION configuration:
    # - Multi-node (3+ replicas): Provides HA + near-zero data loss (~1-2ms write latency)
    # - Single-node (2 instances): Provides data durability + zero-downtime updates (~0.5-1ms latency)
    # - Single instance (1): Remove entire 'synchronous:' section (no replicas to sync)
    #
    # CNPG 1.28+ adds 'failoverQuorum' field for quorum-based failover (opt-in)
    # Example: failoverQuorum: true (under synchronous:)
    synchronous:
      method: ${CNPG_SYNC_METHOD:=any}
      number: ${CNPG_SYNC_NUMBER:=1}
      dataDurability: ${CNPG_SYNC_DURABILITY:=preferred}
    parameters:
      random_page_cost: ${CNPG_RANDOM_PAGE_COST:="1.1"}
      effective_io_concurrency: ${CNPG_EFFECTIVE_IO_CONCURRENCY:="200"}
      # REPLICA CATCH-UP settings (prevents slow WAL archive restore):
      # - wal_keep_size: WAL retained on primary for streaming catch-up (default: 512MB)
      # - max_slot_wal_keep_size: Auto-invalidate slot when replica >N behind â†’ forces pg_basebackup rebuild
      wal_keep_size: ${CNPG_WAL_KEEP_SIZE:="1GB"}
      max_slot_wal_keep_size: ${CNPG_MAX_SLOT_WAL_KEEP:="2GB"}
  monitoring:
    enablePodMonitor: true
