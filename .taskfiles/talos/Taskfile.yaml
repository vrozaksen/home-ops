---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

tasks:
  genconfig:
    desc: Generate Talos configuration
    dir: '{{.TALOS_DIR}}'
    cmds:
      - bws run --no-inherit-env -- talhelper genconfig
    preconditions:
      - bws project list
      - test -f talconfig.yaml
      - test -f talsecret.yaml
      - which talhelper

  apply-node:
    desc: Apply Talos config to a node [HOSTNAME=required] [MODE=auto]
    dir: '{{.TALOS_DIR}}'
    cmds:
      - task: down
      - talosctl apply-config --nodes {{.HOSTNAME}} --mode={{.MODE}} --file clusterconfig/main-{{.HOSTNAME}}.yaml --talosconfig {{.ROOT_DIR}}/talosconfig
      - talosctl --nodes {{.HOSTNAME}} health --talosconfig {{.ROOT_DIR}}/talosconfig
      - task: up
    vars:
      MODE: '{{.MODE | default "auto"}}'
    requires:
      vars:
        - HOSTNAME
    preconditions:
      - talosctl --nodes {{.HOSTNAME}} get machineconfig --talosconfig {{.ROOT_DIR}}/talosconfig
      - test -f talconfig.yaml
      - test -f {{.ROOT_DIR}}/talosconfig

  apply-cluster:
    desc: Apply the Talos config on all nodes for an existing cluster
    vars:
      HOSTNAMES:
        sh: kubectl get nodes --output=jsonpath='{.items[*].metadata.name}'
    cmds:
      - for: {var: HOSTNAMES}
        task: apply-node
        vars:
          HOSTNAME: '{{.ITEM}}'
    preconditions:
      - talosctl config info --talosconfig {{.ROOT_DIR}}/talosconfig
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - test -f {{.ROOT_DIR}}/talosconfig

  soft-nuke:
    desc: Resets nodes back to maintenance mode so you can re-deploy again straight after
    prompt: This will destroy your cluster and reset the nodes back to maintenance mode... continue?
    dir: '{{.TALOS_DIR}}'
    cmd: talhelper gencommand reset --out-dir clusterconfig --config-file talconfig.yaml --extra-flags "--reboot --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL --graceful=false --wait=false" | bash
    preconditions:
      - test -n main

  hard-nuke:
    desc: Resets nodes back completely and reboots them
    prompt: This will destroy your cluster and reset the nodes... continue?
    dir: '{{.TALOS_DIR}}'
    cmd: talhelper gencommand reset --out-dir clusterconfig --config-file talconfig.yaml --extra-flags "--reboot --graceful=false --wait=false" | bash
    preconditions:
      - test -n main

  reboot-node:
    desc: Reboot Talos on a single node [HOSTNAME=required]
    cmds:
      - task: down
      - talosctl --nodes {{.HOSTNAME}} reboot --talosconfig {{.ROOT_DIR}}/talosconfig
      - talosctl --nodes {{.HOSTNAME}} health --talosconfig {{.ROOT_DIR}}/talosconfig
      - task: up
    requires:
      vars:
        - HOSTNAME
    preconditions:
      - talosctl --nodes {{.HOSTNAME}} get machineconfig --talosconfig {{.ROOT_DIR}}/talosconfig
      - talosctl config info --talosconfig {{.ROOT_DIR}}/talosconfig
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - which talosctl

  reboot-cluster:
    desc: Reboot Talos across the whole cluster
    prompt: This will reboot all of the cluster nodes. Are you sure you want to continue?
    vars:
      HOSTNAMES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
    cmds:
      - for: {var: HOSTNAMES}
        task: reboot-node
        vars:
          HOSTNAME: '{{.ITEM}}'
      - task: :kubernetes:cleanse-pods
    preconditions:
      - talosctl config info --talosconfig {{.ROOT_DIR}}/talosconfig
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - test -f {{.ROOT_DIR}}/talosconfig

  shutdown-cluster:
    desc: Shutdown Talos across the whole cluster
    prompt: Shutdown the Talos cluster ... continue?
    cmd: talosctl shutdown --nodes {{.HOSTNAMES}} --force
    vars:
      HOSTNAMES:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
    preconditions:
      - talosctl --nodes {{.NODES}} get machineconfig --talosconfig {{.ROOT_DIR}}/talosconfig
      - talosctl config info --talosconfig {{.ROOT_DIR}}/talosconfig
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - test -f {{.ROOT_DIR}}/talosconfig
      - which talosctl

  kubeconfig:
    desc: Generate the kubeconfig for a Talos cluster
    cmd: talosctl kubeconfig --nodes {{.TALOS_CONTROLLER}} --force {{.ROOT_DIR}}
    vars:
      TALOS_CONTROLLER:
        sh: talosctl --talosconfig {{.ROOT_DIR}}/talosconfig config info --output json --talosconfig {{.ROOT_DIR}}/talosconfig | jq --raw-output '.endpoints[]' | shuf -n 1
    preconditions:
      - talosctl config info --talosconfig {{.ROOT_DIR}}/talosconfig
      - test -f {{.ROOT_DIR}}/talosconfig
      - which talosctl

  down:
    internal: true
    cmds:
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
      - task: :volsync:state-suspend
    preconditions:
      - which kubectl

  up:
    internal: true
    cmds:
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
      - task: :volsync:state-resume
    preconditions:
      - which kubectl
