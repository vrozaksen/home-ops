# Plan Migracji z Crunchy Postgres do CloudNative-PG

**Data utworzenia:** 7 listopada 2025
**Status:** Planowanie

## üîç Analiza Obecnego Stanu

### Aplikacje Aktywne (14 aplikacji u≈ºywajƒÖcych Crunchy Postgres):

#### AI (2):
- `litellm`
- `open-webui`

#### Self-hosted (3):
- `atuin`
- `reactive-resume`
- `miniflux`

#### Security (1):
- `authentik` ‚ö†Ô∏è **KRYTYCZNA**

#### Observability (1):
- `grafana` ‚ö†Ô∏è **KRYTYCZNA**

#### Media (2):
- `jellyseerr`
- `jellystat`

#### Downloads - ARR Stack (4):
- `sonarr`
- `radarr`
- `prowlarr`
- `bazarr`

#### Sentry (1):
- `sentry`

### Aplikacje Zarchiwizowane (6 aplikacji):
- `vikunja` ‚úÖ Kandydat do migracji
- `paperless` ‚úÖ Kandydat do migracji (du≈ºa baza!)
- `mealie` ‚úÖ Kandydat do migracji
- `zipline` ‚ùå Niski priorytet
- `netbox` ‚ö†Ô∏è Z≈Ço≈ºona konfiguracja
- `outline` ‚ö†Ô∏è Mo≈ºe byƒá stary chart

---

## üîÑ G≈Ç√≥wne R√≥≈ºnice miƒôdzy Crunchy i CNPG

| Aspekt | Crunchy Postgres | CloudNative-PG |
|--------|-----------------|----------------|
| **Nazwa klastra** | `${APP}` | `postgres-${APP}` |
| **Secret u≈ºytkownika** | `${APP}-pguser-${APP}` | `postgres-${APP}-app` |
| **Klucz has≈Ça w secret** | `password` | `password` (bez zmian) |
| **Klucz URI** | `uri` | `uri` (bez zmian) |
| **Service endpoint** | `${APP}-pgbouncer` | `postgres-${APP}-rw` |
| **PgBouncer** | Wbudowany | Opcjonalny external |
| **Backup** | pgBackRest (czƒôste incr.) | Barman Cloud (daily) |
| **Postgres wersja** | 17 | 17-18 (konfigurowalny) |
| **Repliki** | 3 (domy≈õlnie) | 3 (domy≈õlnie) |
| **Storage domy≈õlny** | 5Gi | 2Gi |
| **CPU domy≈õlny** | brak limitu | 25m request |
| **Memory domy≈õlny** | brak limitu | 256Mi/512Mi |
| **Max connections** | 500 | 100 |

---

## üìã Plan Migracji Krok po Kroku

### FAZA 0: Przygotowania (PRE-MIGRATION)

#### 1. Backup wszystkich aktywnych baz ‚úÖ
```bash
# Sprawdziƒá status backup√≥w w MinIO
# Crunchy automatycznie tworzy backupy do MinIO
kubectl get cronjobs -n database | grep repo

# Zrobiƒá manual full backup przed migracjƒÖ dla ka≈ºdej aplikacji
kubectl annotate postgrescluster <APP> -n <NAMESPACE> \
  postgres-operator.crunchydata.com/pgbackrest-backup="$(date +%Y%m%d-%H%M%S)"
```

#### 2. Dokumentacja po≈ÇƒÖcze≈Ñ aplikacji
- Zweryfikowaƒá kt√≥re aplikacje u≈ºywajƒÖ PgBouncer
- Kt√≥re majƒÖ direct connection do Postgres
- Sprawdziƒá ENV variables w ka≈ºdej aplikacji

#### 3. Weryfikacja External Secrets
```bash
# Sprawdziƒá czy secret cloudnative-pg istnieje
kubectl get externalsecret cloudnative-pg -n database

# Bitwarden secret: 'cloudnative-pg' powinien zawieraƒá:
# - POSTGRES_SUPER_USER
# - POSTGRES_SUPER_PASS
# - AWS_ACCESS_KEY_ID
# - AWS_SECRET_ACCESS_KEY
```

---

### FAZA 1: Infrastruktura

#### 1. Upewniƒá siƒô ≈ºe CNPG operator dzia≈Ça
```bash
kubectl get pods -n database -l app.kubernetes.io/name=cloudnative-pg
kubectl logs -n database -l app.kubernetes.io/name=cloudnative-pg
```

#### 2. Sprawdziƒá plugin barman-cloud
```bash
kubectl get pods -n database -l app=plugin-barman-cloud
```

#### 3. Zweryfikowaƒá dostƒôp do MinIO/S3
```bash
# Test po≈ÇƒÖczenia do bucket'a postgresql
kubectl run -it --rm test-s3 --image=amazon/aws-cli --restart=Never -- \
  s3 ls s3://postgresql/ --endpoint-url=https://s3.vzkn.eu
```

---

### FAZA 2: Strategia Migracji

**OPCJA A: Migracja "na czysto" (ZALECANA)**
- Dump z Crunchy ‚Üí Import do CNPG
- Kontrolowany downtime (~5-15 min per app)
- Czysta konfiguracja CNPG od poczƒÖtku
- **Wybrana metoda dla wszystkich aplikacji**

**OPCJA B: Migracja przez restore z backup**
- Wykorzystanie Crunchy backup ‚Üí CNPG restore
- Trudniejsza konwersja format√≥w (pgBackRest ‚Üí Barman)
- **NIE ZALECANA** - r√≥≈ºne narzƒôdzia backup

---

### FAZA 3: Kolejno≈õƒá Migracji

#### Grupa 1 - Niski priorytet (DEV/TEST)
**Cel:** Zdobycie do≈õwiadczenia, testowanie procedury

1. **atuin** - ma≈Ça baza, niski ruch, osobiste u≈ºycie
2. **miniflux** - RSS reader, mo≈ºna straciƒá status przeczytanych
3. **reactive-resume** - CV builder, ma≈Ço danych

**Downtime akceptowalny:** 30-60 min

---

#### Grupa 2 - Media (≈õredni priorytet)
**Cel:** Aplikacje z wiƒôkszym ruchem, ale nie krytyczne

4. **jellystat** - tylko statystyki, mo≈ºna odtworzyƒá
5. **jellyseerr** - requesty medi√≥w, akceptowalny downtime

**Downtime akceptowalny:** 15-30 min

---

#### Grupa 3 - ARR Stack (wy≈ºszy priorytet)
**Cel:** Aplikacje powiƒÖzane, migrowaƒá razem w oknie maintenance

6. **prowlarr** - najpierw indexer (inne zale≈ºƒÖ od niego)
7. **bazarr** - napisy, najmniej krytyczny z ARR
8. **radarr** - filmy
9. **sonarr** - seriale

**Downtime akceptowalny:** 15 min per app, ~1h ca≈Ço≈õƒá
**Zalecenie:** Weekend, p√≥≈∫ny wiecz√≥r

---

#### Grupa 4 - AI/ML (≈õredni-wysoki priorytet)

10. **litellm** - proxy LLM, u≈ºywany aktywnie
11. **open-webui** - UI dla LLM

**Downtime akceptowalny:** 10-15 min

---

#### Grupa 5 - Krytyczne (ostatnie, najwiƒôksza ostro≈ºno≈õƒá)
**Cel:** Po zdobyciu do≈õwiadczenia z poprzednimi

12. **grafana** - dashboardy i alerty, monitoring ‚ö†Ô∏è
13. **authentik** - SSO/Identity Provider ‚ö†Ô∏è‚ö†Ô∏è **NAJWA≈ªNIEJSZE**
14. **sentry** - error tracking

**Downtime akceptowalny:** <10 min
**Zalecenie:**
- Grafana: weekend, rano (≈ºeby monitorowaƒá przez dzie≈Ñ)
- Authentik: **weekend, z backup planem**
- Sentry: po Authentik, ≈ºeby mieƒá error tracking

**üí° NOTA:** Synchronous replication (`CNPG_SYNC_METHOD: any`) bƒôdzie **domy≈õlnie w≈ÇƒÖczona** dla WSZYSTKICH aplikacji - minimalne overhead (~1-2ms), zero data loss na failover.

---

### FAZA 4: Procedura Migracji dla Pojedynczej Aplikacji

#### Szablon dla aplikacji: `<APP>`

```bash
# ==============================================================================
# MIGRACJA: <APP>
# Namespace: <NAMESPACE>
# Data: <DATA>
# ==============================================================================

# --- KROK 1: PRE-MIGRATION BACKUP ---
echo "==> Tworzenie backup przed migracjƒÖ..."

# Manual backup w Crunchy
kubectl annotate postgrescluster <APP> -n <NAMESPACE> \
  postgres-operator.crunchydata.com/pgbackrest-backup="$(date +%Y%m%d-%H%M%S)"

# Poczekaj na zako≈Ñczenie backup jobu
kubectl wait --for=condition=complete job -l postgres-operator.crunchydata.com/cluster=<APP> \
  -n <NAMESPACE> --timeout=30m

# --- KROK 2: SCALE DOWN APLIKACJI ---
echo "==> Zatrzymywanie aplikacji..."
kubectl scale deployment <APP> -n <NAMESPACE> --replicas=0

# Poczekaj na zatrzymanie
kubectl wait --for=delete pod -l app.kubernetes.io/name=<APP> -n <NAMESPACE> --timeout=5m

# --- KROK 3: DUMP BAZY DANYCH ---
echo "==> Dump bazy danych..."

# Znajd≈∫ pod Postgres
POSTGRES_POD=$(kubectl get pods -n <NAMESPACE> -l postgres-operator.crunchydata.com/cluster=<APP>,postgres-operator.crunchydata.com/role=master -o jsonpath='{.items[0].metadata.name}')
echo "Pod: $POSTGRES_POD"

# Dump do pliku
kubectl exec -n <NAMESPACE> $POSTGRES_POD -- \
  pg_dump -U <APP> -d <APP> -F c -f /tmp/<APP>.dump

# Skopiuj dump lokalnie
kubectl cp <NAMESPACE>/$POSTGRES_POD:/tmp/<APP>.dump ./<APP>.dump

# Backup dump do bezpiecznej lokacji
cp ./<APP>.dump ./backups/<APP>-$(date +%Y%m%d-%H%M%S).dump

# --- KROK 4: AKTUALIZACJA KONFIGURACJI ---
echo "==> Aktualizacja konfiguracji Kubernetes..."

# Edytuj: kubernetes/apps/<NAMESPACE>/<APP>/ks.yaml
# ZMIANY:
# 1. components:
#    BY≈ÅO: - ../../../components/postgres
#    JEST: - ../../../components/cnpg/restore  # ‚úÖ PERMANENTNY!
#
# 2. dependsOn:
#    BY≈ÅO: - name: crunchy-postgres-operator
#    JEST: - name: cloudnative-pg
#          - name: plugin-barman-cloud
#
# 3. healthCheckExprs:
#    BY≈ÅO: - apiVersion: postgres-operator.crunchydata.com/v1beta1
#          kind: PostgresCluster
#          failed: status.conditions.filter(e, e.type == 'ProxyAvailable').all(e, e.status == 'False')
#          current: status.conditions.filter(e, e.type == 'ProxyAvailable').all(e, e.status == 'True')
#    JEST: - apiVersion: postgresql.cnpg.io/v1
#          kind: Cluster
#          failed: status.conditions.filter(e, e.type == 'Ready').all(e, e.status == 'False')
#          current: status.conditions.filter(e, e.type == 'Ready').all(e, e.status == 'True')
#
# 4. postBuild.substitute (dodaƒá):
#    APP: <APP>
#    CNPG_SIZE: 5Gi  # dostosowaƒá do rozmiaru bazy
#    CNPG_REQUESTS_CPU: 100m  # dostosowaƒá
#    CNPG_LIMITS_MEMORY: 512Mi  # dostosowaƒá
#
# UWAGA: U≈ºywamy 'restore' nie 'initdb' bo migrujemy ISTNIEJƒÑCE bazy!
#        Po migracji ZOSTAW 'restore' na sta≈Çe dla disaster recovery!

# Edytuj: kubernetes/apps/<NAMESPACE>/<APP>/helmrelease.yaml
# ZMIANY w ENV:
# 1. Secret name:
#    BY≈ÅO: <APP>-pguser-<APP>
#    JEST: postgres-<APP>-app
#
# 2. Database host/service (je≈õli u≈ºywane):
#    BY≈ÅO: <APP>-pgbouncer.<NAMESPACE>.svc
#    JEST: postgres-<APP>-rw.<NAMESPACE>.svc
#
# 3. Klucz has≈Ça (bez zmian): password
# 4. Klucz URI (bez zmian): uri

# --- KROK 5: COMMIT I DEPLOY ---
echo "==> Commit zmian..."
git add kubernetes/apps/<NAMESPACE>/<APP>/
git commit -m "migrate(<APP>): Crunchy Postgres ‚Üí CloudNative-PG"
git push

# --- KROK 6: WAIT FOR CNPG CLUSTER ---
echo "==> Czekam na CNPG cluster..."
kubectl wait --for=condition=Ready cluster/postgres-<APP> -n <NAMESPACE> --timeout=10m

# Sprawd≈∫ status
kubectl get cluster postgres-<APP> -n <NAMESPACE>
kubectl get pods -n <NAMESPACE> -l cnpg.io/cluster=postgres-<APP>

# --- KROK 7: RESTORE DUMP ---
echo "==> Restore danych..."

# Znajd≈∫ primary pod CNPG
CNPG_POD=$(kubectl get pods -n <NAMESPACE> -l cnpg.io/cluster=postgres-<APP>,role=primary -o jsonpath='{.items[0].metadata.name}')
echo "CNPG Pod: $CNPG_POD"

# Skopiuj dump do poda
kubectl cp ./<APP>.dump <NAMESPACE>/$CNPG_POD:/tmp/<APP>.dump

# Restore dump
kubectl exec -n <NAMESPACE> $CNPG_POD -- \
  pg_restore -U <APP> -d <APP> -c -F c /tmp/<APP>.dump

# Cleanup dump w podzie
kubectl exec -n <NAMESPACE> $CNPG_POD -- rm /tmp/<APP>.dump

# --- KROK 8: VERIFY DATABASE ---
echo "==> Weryfikacja bazy danych..."

# Sprawd≈∫ tabele
kubectl exec -n <NAMESPACE> $CNPG_POD -- \
  psql -U <APP> -d <APP> -c "\dt"

# Sprawd≈∫ przyk≈Çadowe dane (dostosowaƒá query)
kubectl exec -n <NAMESPACE> $CNPG_POD -- \
  psql -U <APP> -d <APP> -c "SELECT COUNT(*) FROM <main_table>;"

# --- KROK 9: SCALE UP APLIKACJI ---
echo "==> Uruchamianie aplikacji..."
kubectl scale deployment <APP> -n <NAMESPACE> --replicas=1

# Poczekaj na ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=<APP> -n <NAMESPACE> --timeout=5m

# --- KROK 10: WERYFIKACJA APLIKACJI ---
echo "==> Weryfikacja aplikacji..."

# Sprawd≈∫ logi
kubectl logs -n <NAMESPACE> -l app.kubernetes.io/name=<APP> --tail=100

# Sprawd≈∫ czy aplikacja dzia≈Ça (curl do healthcheck endpoint)
kubectl run -it --rm test-curl --image=curlimages/curl --restart=Never -- \
  curl -v http://<APP>.<NAMESPACE>.svc/<healthcheck-path>

# --- KROK 11: POST-MIGRATION MONITORING ---
echo "==> Monitoring przez 24h..."
echo "1. Sprawd≈∫ logi aplikacji przez pierwsze 30 min"
echo "2. Test funkcjonalno≈õci krytycznych"
echo "3. Monitor resource usage w Grafana"
echo "4. Sprawd≈∫ czy backup CNPG dzia≈Ça (nastƒôpnego dnia)"

# --- KROK 12: CLEANUP (PO 24-48H) ---
echo "==> Cleanup po pomy≈õlnej migracji..."

# Usu≈Ñ stary PostgresCluster (Crunchy)
# kubectl delete postgrescluster <APP> -n <NAMESPACE>

# Usu≈Ñ stare PVC (OPCJONALNIE, zachowaƒá backup przez tydzie≈Ñ)
# kubectl get pvc -n <NAMESPACE> | grep <APP>-postgres
# kubectl delete pvc <APP>-postgres-<instance>-xxxxx -n <NAMESPACE>

echo "==> MIGRACJA ZAKO≈ÉCZONA"
```

---

### FAZA 5: Szczeg√≥≈Çowe Zmiany w Konfiguracji

#### Przyk≈Çad: prowlarr

**PRZED (Crunchy):**
```yaml
# kubernetes/apps/downloads/prowlarr/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app prowlarr
spec:
  components:
    - ../../../components/postgres
  dependsOn:
    - name: crunchy-postgres-operator
      namespace: database
    - name: external-secrets-store
      namespace: external-secrets
  healthCheckExprs:
    - apiVersion: postgres-operator.crunchydata.com/v1beta1
      kind: PostgresCluster
      failed: status.conditions.filter(e, e.type == 'ProxyAvailable').all(e, e.status == 'False')
      current: status.conditions.filter(e, e.type == 'ProxyAvailable').all(e, e.status == 'True')
  interval: 1h
  path: ./kubernetes/apps/downloads/prowlarr
  postBuild:
    substitute:
      APP: *app
  prune: true
  sourceRef:
    kind: GitRepository
    name: flux-system
    namespace: flux-system
  wait: false
```

**PO (CNPG):**
```yaml
# kubernetes/apps/downloads/prowlarr/ks.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: &app prowlarr
spec:
  components:
    - ../../../components/cnpg/backup
  dependsOn:
    - name: cloudnative-pg
      namespace: database
    - name: external-secrets-store
      namespace: external-secrets
  healthCheckExprs:
    - apiVersion: postgresql.cnpg.io/v1
      kind: Cluster
      failed: status.conditions.filter(e, e.type == 'Ready').all(e, e.status == 'False')
      current: status.conditions.filter(e, e.type == 'Ready').all(e, e.status == 'True')
  interval: 1h
  path: ./kubernetes/apps/downloads/prowlarr
  postBuild:
    substitute:
      APP: *app
      CNPG_SIZE: 2Gi
      CNPG_REQUESTS_CPU: 100m
      CNPG_LIMITS_MEMORY: 512Mi
  prune: true
  sourceRef:
    kind: GitRepository
    name: flux-system
    namespace: flux-system
  wait: false
```

**HelmRelease ENV:**
```yaml
# PRZED (Crunchy)
env:
  - name: POSTGRES__HOST
    value: prowlarr-pgbouncer.downloads.svc
  - name: POSTGRES__PASSWORD
    valueFrom:
      secretKeyRef:
        name: prowlarr-pguser-prowlarr
        key: password

# PO (CNPG) - opcja 1: URI
env:
  - name: POSTGRES__URI
    valueFrom:
      secretKeyRef:
        name: postgres-prowlarr-app
        key: uri

# PO (CNPG) - opcja 2: individual fields
env:
  - name: POSTGRES__HOST
    valueFrom:
      secretKeyRef:
        name: postgres-prowlarr-app
        key: host
  - name: POSTGRES__PASSWORD
    valueFrom:
      secretKeyRef:
        name: postgres-prowlarr-app
        key: password
```

---

### FAZA 6: Resource Sizing Guidelines

#### Zalecenia per aplikacja:

| Aplikacja | Baza Size | CPU Request | Memory Limit | Max Conn | Uzasadnienie |
|-----------|-----------|-------------|--------------|----------|--------------|
| atuin | 2Gi | 25m | 256Mi | 50 | Ma≈Ça baza, osobiste |
| miniflux | 2Gi | 50m | 512Mi | 100 | RSS, ≈õredni ruch |
| reactive-resume | 2Gi | 25m | 256Mi | 50 | Ma≈Ço danych |
| jellystat | 2Gi | 100m | 512Mi | 100 | Statystyki, analytics |
| jellyseerr | 2Gi | 100m | 512Mi | 100 | Requesty |
| prowlarr | 2Gi | 100m | 512Mi | 100 | Indexer, API calls |
| bazarr | 2Gi | 100m | 512Mi | 100 | Napisy |
| radarr | 3Gi | 200m | 1Gi | 150 | Du≈ºa biblioteka |
| sonarr | 3Gi | 200m | 1Gi | 150 | Du≈ºa biblioteka |
| litellm | 2Gi | 100m | 512Mi | 100 | Proxy LLM |
| open-webui | 3Gi | 200m | 1Gi | 150 | UI LLM, historia |
| grafana | 5Gi | 200m | 1Gi | 200 | Dashboardy, alerty |
| authentik | 5Gi | 500m | 2Gi | 300 | SSO, krytyczna |
| sentry | 5Gi | 300m | 1Gi | 200 | Error tracking |

**üí° Wszystkie u≈ºywajƒÖ sync replication (domy≈õlnie):**
- `CNPG_SYNC_METHOD: any, CNPG_SYNC_NUMBER: '1', CNPG_SYNC_DURABILITY: preferred`
- Minimalne overhead, near-zero data loss, self-healing

---

### FAZA 7: Migracja Aplikacji Zarchiwizowanych

#### Decyzja per aplikacja:

**DO MIGRACJI (teraz):**
1. ‚úÖ **mealie** - recipes, popularne, prawdopodobne odarchiwizowanie
2. ‚úÖ **vikunja** - TODO manager, mo≈ºe wr√≥ciƒá
3. ‚úÖ **paperless** - dokumenty, DU≈ªA BAZA!, wa≈ºne dane

**DO MIGRACJI (przy odarchiwizowaniu):**
4. ‚ö†Ô∏è **netbox** - z≈Ço≈ºona, zmigrowaƒá tylko je≈õli pewno≈õƒá odarchiwizowania
5. ‚ö†Ô∏è **outline** - wiki, mo≈ºe wymagaƒá update chart'u
6. ‚ùå **zipline** - niski priorytet, likely deprecated

#### Procedura dla zarchiwizowanych:

```bash
# 1. Backup istniejƒÖcej bazy Crunchy (je≈õli jeszcze dzia≈Ça)
# 2. Zmigruj konfiguracjƒô (ks.yaml, helmrelease.yaml)
# 3. DODAJ komentarz w .archive/*/ks.yaml:
#    # MIGRATED TO CNPG - READY TO RESTORE FROM CRUNCHY BACKUP
# 4. Przy odarchiwizowaniu:
#    - Je≈õli backup istnieje: u≈ºyj components/cnpg/restore
#    - Je≈õli brak backup: fresh start
```

---

### FAZA 8: Cleanup po Pe≈Çnej Migracji

Po pomy≈õlnej migracji WSZYSTKICH aplikacji (14 aktywnych):

```bash
# 1. Usu≈Ñ wszystkie stare PostgresCluster (Crunchy)
kubectl get postgrescluster --all-namespaces
# Usu≈Ñ ka≈ºdy rƒôcznie po weryfikacji

# 2. Usu≈Ñ Crunchy operator
# Edytuj: kubernetes/apps/database/kustomization.yaml
# Usu≈Ñ liniƒô: - ./crunchy-postgres/ks.yaml

# 3. Commit
git add kubernetes/apps/database/kustomization.yaml
git commit -m "chore(database): remove Crunchy Postgres operator after full migration"
git push

# 4. Usu≈Ñ katalog components/postgres (opcjonalnie)
# Zachowaƒá przez 1-2 tygodnie dla rollback

# 5. Update dokumentacji
# Zaktualizuj README.md w komponencie CNPG
```

---

## ‚ö†Ô∏è Ryzyka i Mitigation

### Ryzyko 1: Authentik down = brak dostƒôpu do wszystkiego
**Mitigation:**
- Migruj w weekend z zapasowym planem
- Miej gotowy rollback (Crunchy cluster backup)
- Test login PRZED i PO migracji
- Backup Authentik secrets osobno
- Synchronous replication **domy≈õlnie w≈ÇƒÖczona** dla wszystkich aplikacji (minimal overhead, zero data loss)

### Ryzyko 2: Strata danych podczas dump/restore
**Mitigation:**
- Zawsze manual backup przed migracjƒÖ
- Test dump przed delete Crunchy
- Weryfikacja count(*) po restore
- Zachowaƒá Crunchy cluster 48h po migracji

### Ryzyko 3: Aplikacja nie dzia≈Ça z CNPG
**Mitigation:**
- Test na dev aplikacji najpierw (atuin)
- Sprawd≈∫ logi aplikacji
- Rollback: skaluj down, przywr√≥ƒá Crunchy, skaluj up

### Ryzyko 4: Storage za ma≈Çy (2Gi domy≈õlne)
**Mitigation:**
- Sprawd≈∫ rozmiar obecnych baz:
  ```bash
  kubectl exec -n <NS> <POD> -- \
    psql -U postgres -c "SELECT pg_database.datname, pg_size_pretty(pg_database_size(pg_database.datname)) FROM pg_database;"
  ```
- Ustaw CNPG_SIZE na 2x obecny rozmiar minimum

### Ryzyko 5: Max connections za niski (100 vs 500)
**Mitigation:**
- Monitoruj connection count przed migracjƒÖ
- Zwiƒôksz CNPG_MAX_CONNECTIONS je≈õli potrzeba (ale idle DBs zwykle ok z 100)
- ‚ÑπÔ∏è PgBouncer nie jest potrzebny - failover automatyczny przez `-rw` service

### Ryzyko 6: Backup nie dzia≈Ça w CNPG
**Mitigation:**
- Test backup/restore przed migracjƒÖ produkcji
- Sprawd≈∫ ScheduledBackup status:
  ```bash
  kubectl get scheduledbackup --all-namespaces
  ```
- Verify backups w MinIO bucket

---

## üìù Checklist Master (ca≈Ço≈õƒá projektu)

### Pre-Migration
- [ ] Backup wszystkich 14 baz danych Crunchy
- [ ] Zweryfikowaƒá CNPG operator ready
- [ ] Zweryfikowaƒá barman-cloud plugin ready
- [ ] Zweryfikowaƒá External Secrets CNPG
- [ ] Test dump/restore na dev aplikacji
- [ ] Przygotowaƒá rollback plan

### Migration Progress

#### Grupa 1 - DEV/TEST
- [ ] atuin (namespace: self-hosted)
- [ ] miniflux (namespace: self-hosted)
- [ ] reactive-resume (namespace: self-hosted)

#### Grupa 2 - Media
- [ ] jellystat (namespace: media)
- [ ] jellyseerr (namespace: media)

#### Grupa 3 - ARR Stack
- [ ] prowlarr (namespace: downloads)
- [ ] bazarr (namespace: downloads)
- [ ] radarr (namespace: downloads)
- [ ] sonarr (namespace: downloads)

#### Grupa 4 - AI
- [ ] litellm (namespace: ai)
- [ ] open-webui (namespace: ai)

#### Grupa 5 - Critical
- [ ] grafana (namespace: observability) ‚ö†Ô∏è
- [ ] authentik (namespace: security) ‚ö†Ô∏è‚ö†Ô∏è
- [ ] sentry (namespace: sentry) ‚ö†Ô∏è

### Post-Migration
- [ ] Monitor wszystkich aplikacji 24-48h
- [ ] Verify backups CNPG dzia≈ÇajƒÖ
- [ ] Test restore z CNPG backup (sample app)
- [ ] Cleanup stare PostgresCluster Crunchy
- [ ] Cleanup stare PVC
- [ ] Remove Crunchy operator
- [ ] Update dokumentacji

### Archived Apps (optional)
- [ ] mealie
- [ ] vikunja
- [ ] paperless

---

## üìä Tracking Progress

| # | Aplikacja | Namespace | Status | Data | Downtime | Notes |
|---|-----------|-----------|--------|------|----------|-------|
| 1 | atuin | self-hosted | ‚è≥ Pending | - | - | - |
| 2 | miniflux | self-hosted | ‚è≥ Pending | - | - | - |
| 3 | reactive-resume | self-hosted | ‚è≥ Pending | - | - | - |
| 4 | jellystat | media | ‚è≥ Pending | - | - | - |
| 5 | jellyseerr | media | ‚è≥ Pending | - | - | - |
| 6 | prowlarr | downloads | ‚è≥ Pending | - | - | - |
| 7 | bazarr | downloads | ‚è≥ Pending | - | - | - |
| 8 | radarr | downloads | ‚è≥ Pending | - | - | - |
| 9 | sonarr | downloads | ‚è≥ Pending | - | - | - |
| 10 | litellm | ai | ‚è≥ Pending | - | - | - |
| 11 | open-webui | ai | ‚è≥ Pending | - | - | - |
| 12 | grafana | observability | ‚è≥ Pending | - | - | - |
| 13 | authentik | security | ‚è≥ Pending | - | - | - |
| 14 | sentry | sentry | ‚è≥ Pending | - | - | - |

**Status legend:**
- ‚è≥ Pending - Nie rozpoczƒôto
- üîÑ In Progress - W trakcie migracji
- ‚úÖ Completed - Zako≈Ñczona pomy≈õlnie
- ‚ùå Failed - Niepowodzenie (wymaga rollback)
- üîô Rolled Back - Wycofano zmiany

---

## üéØ Podsumowanie

### Oszacowany czas:
- **Przygotowania:** 1-2h
- **Migracja pojedynczej aplikacji:** 15-30 min
- **Ca≈Ço≈õƒá (14 aplikacji):** ~8-12h roboczych (roz≈Ço≈ºone na 3-5 dni)

### Zalecenia:
1. ‚úÖ Zacznij od `atuin` (najmniejsze ryzyko)
2. ‚úÖ Migruj max 2-3 aplikacje dziennie
3. ‚úÖ Testuj dok≈Çadnie po ka≈ºdej migracji (24h monitoring)
4. ‚úÖ Zostaw `authentik` i `grafana` na koniec
5. ‚úÖ Weekend dla krytycznych aplikacji
6. ‚úÖ Zachowaj Crunchy clusters 48h po migracji
7. ‚úÖ Backup przed ka≈ºdƒÖ migracjƒÖ

### Success Criteria:
- ‚úÖ Wszystkie aplikacje dzia≈ÇajƒÖ poprawnie
- ‚úÖ Dane zmigrowane bez strat
- ‚úÖ Backups CNPG dzia≈ÇajƒÖ
- ‚úÖ Resource usage w normie
- ‚úÖ Brak b≈Çƒôd√≥w w logach aplikacji
- ‚úÖ Performance por√≥wnywalny lub lepszy
- ‚úÖ Crunchy operator usuniƒôty

### Rollback Plan:
Je≈õli migracja nie powiedzie siƒô:
1. Scale down aplikacjƒô
2. Przywr√≥ƒá stary PostgresCluster (Crunchy)
3. Restore z Crunchy backup
4. Przywr√≥ƒá stare referencje w ks.yaml/helmrelease.yaml
5. Scale up aplikacjƒô
6. Debug problem przed kolejnƒÖ pr√≥bƒÖ

---

## üìö Referencje

- **CNPG Docs:** https://cloudnative-pg.io/
- **Crunchy Docs:** https://access.crunchydata.com/documentation/postgres-operator/
- **Migration Guide:** `/home/vrozaksen/git/home-ops/kubernetes/components/cnpg/README.md`
- **Resource Config:** `/home/vrozaksen/git/home-ops/docs/cnpg-resource-configs.yaml`

---

**Ostatnia aktualizacja:** 7 listopada 2025
**Autor:** Migration Team
**Status:** Ready for Execution
