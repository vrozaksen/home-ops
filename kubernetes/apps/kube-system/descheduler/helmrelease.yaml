---
# yaml-language-server: $schema=https://schemas.vzkn.eu/source.toolkit.fluxcd.io/ocirepository_v1.json
apiVersion: source.toolkit.fluxcd.io/v1
kind: OCIRepository
metadata:
  name: descheduler
spec:
  interval: 5m
  layerSelector:
    mediaType: application/vnd.cncf.helm.chart.content.v1.tar+gzip
    operation: copy
  ref:
    tag: 0.34.0
  url: oci://ghcr.io/home-operations/charts-mirror/descheduler
---
# yaml-language-server: $schema=https://schemas.vzkn.eu/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: descheduler
spec:
  chartRef:
    kind: OCIRepository
    name: descheduler
  interval: 1h
  values:
    kind: Deployment
    deschedulerPolicyAPIVersion: descheduler/v1alpha2
    deschedulerPolicy:
      # Global eviction limits to prevent disruption
      maxNoOfPodsToEvictPerNode: 10
      maxNoOfPodsToEvictPerNamespace: 5
      # Use actual metrics from metrics-server instead of requests
      metricsProviders:
        - source: KubernetesMetrics
      profiles:
        - name: Default
          pluginConfig:
            # Pod protection and eviction filtering
            - name: DefaultEvictor
              args:
                # Verify pods can be scheduled elsewhere before eviction
                nodeFit: true
                # Protect pods younger than 5 minutes (prevents thrashing)
                minPodAge: 5m
                # Allow single-replica apps to be moved (they restart anyway)
                minReplicas: 1
                # Pod protections
                podProtections:
                  defaultDisabled:
                    - FailedBarePods
                    - PodsWithLocalStorage
                    - SystemCriticalPods
                # Don't evict critical system pods
                priorityThreshold:
                  value: 100000000
            # Balance nodes using actual metrics from metrics-server
            - name: LowNodeUtilization
              args:
                metricsUtilization:
                  source: KubernetesMetrics
                # Thresholds for 3 CP + 3 worker topology
                # Workers: CPU 14-24%, Memory 25-38%
                # CP nodes excluded via taint (nodeFit: true)
                thresholds:
                  cpu: 15
                  memory: 30
                  pods: 20
                # Target thresholds - nodes above these are overutilized
                targetThresholds:
                  cpu: 50
                  memory: 65
                  pods: 60
                # Skip these namespaces from eviction
                evictableNamespaces:
                  exclude:
                    - kube-system
                    - rook-ceph
                    - flux-system
            # Ensure replicas are spread across nodes (not co-located)
            - name: RemoveDuplicates
            # Clean up failed pods
            - name: RemoveFailedPods
              args:
                reasons:
                  - ContainerStatusUnknown
                  - NodeAffinity
                  - NodeShutdown
                  - Terminated
                  - UnexpectedAdmissionError
                includingInitContainers: true
                excludeOwnerKinds:
                  - Job
                minPodLifetimeSeconds: 1800
            - name: RemovePodsViolatingInterPodAntiAffinity
            - name: RemovePodsViolatingNodeAffinity
              args:
                nodeAffinityType:
                  - requiredDuringSchedulingIgnoredDuringExecution
            - name: RemovePodsViolatingNodeTaints
            - name: RemovePodsViolatingTopologySpreadConstraint
              args:
                # Only enforce hard constraints
                constraints:
                  - DoNotSchedule
          plugins:
            balance:
              enabled:
                - LowNodeUtilization
                - RemoveDuplicates
                - RemovePodsViolatingTopologySpreadConstraint
            deschedule:
              enabled:
                - RemoveFailedPods
                - RemovePodsViolatingInterPodAntiAffinity
                - RemovePodsViolatingNodeAffinity
                - RemovePodsViolatingNodeTaints
    service:
      enabled: true
    serviceMonitor:
      enabled: true
    leaderElection:
      enabled: true
    # Schedule on control-plane nodes
    nodeSelector:
      node-role.kubernetes.io/control-plane: ""
    tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
